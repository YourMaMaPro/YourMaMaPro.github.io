## Theoretical advances

[Neural Kernels without Tangents](https://proceedings.icml.cc/static/paper_files/icml/2020/3479-Paper.pdf)  

[The Recurrent Neural Tangent Kernel](https://arxiv.org/pdf/2006.10246.pdf)  

[A bio-inspired bistable recurrent cell allows forlong-lasting memory](https://arxiv.org/pdf/2006.05252.pdf)  

[mixup: BEYONDEMPIRICALRISKMINIMIZATION](https://arxiv.org/pdf/1710.09412.pdf)  

[On Mixup Regularization](https://arxiv.org/pdf/2006.06049.pdf)  

[Neural Networks and Physical Systems with Emergent Collective Computational Abilities](https://bi.snu.ac.kr/Courses/g-ai09-2/hopfield82.pdf)  

[Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf)  

[mplicit Generation and Modeling with Energy-BasedModels](https://papers.nips.cc/paper/8619-implicit-generation-and-modeling-with-energy-based-models.pdf)  

[LEARNINGMIXED-CURVATUREREPRESENTATIONS INPRODUCTS OFMODELSPACES](https://openreview.net/pdf?id=HJxeWnCcF7)  

[Toward Deeper Understanding of Neural Networks: The Powerof Initialization and a Dual View on Expressivity](http://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity.pdf)  

[Neural Tangent Kernel:Convergence and Generalization in Neural Networks](https://arxiv.org/pdf/1806.07572.pdf)  

[Gaussian Process Behaviour in Wide Deep Neural Networks](https://arxiv.org/pdf/1804.11271.pdf)

[Dynamical Isometry and a Mean Field Theory of CNNs:How to Train 10,000-Layer Vanilla Convolutional Neural Networks](https://arxiv.org/pdf/1806.05393.pdf)  

[DEEPCONVOLUTIONALNETWORKS ASSHALLOWGAUSSIANPROCESSES](https://arxiv.org/pdf/1808.05587.pdf)  

[Neural Tangents Github](https://github.com/google/neural-tangents)

[Geometric deep learning:going beyond Euclidean data](https://arxiv.org/pdf/1611.08097.pdf)  

[TRADI: Tracking deep neural network weightdistributions](https://arxiv.org/pdf/1912.11316.pdf)  

[Bayesian learning for neural networks](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&rep=rep1&type=pdf)

## Graph networks

[Dynamic Factor Graphsfor Time Series Modeling](http://yann.lecun.com/exdb/publis/pdf/mirowski-ecml-09.pdf)  

[The Temporal Event Graph](https://arxiv.org/pdf/1706.02128.pdf)  

[Graph Neural Networks:A Review of Methods and Applications](https://arxiv.org/pdf/1812.08434.pdf)  

[Towards understanding glasses with graph neural networks](https://deepmind.com/blog/article/Towards-understanding-glasses-with-graph-neural-networks)  


## Modeling time series

[Phd Time Series Modelingwith Hidden Variablesand Gradient-Based Algorithms](https://cs.nyu.edu/media/publications/mirowski_piotr.pdf)  

[Machine Learning Methods for Detecting Rare Eventsin Temporal Data](https://mediatum.ub.tum.de/doc/1444158/1444158.pdf)  

[Untangling tradeoffs between recurrence andself-attention in neural networks](https://arxiv.org/pdf/2006.09471.pdf)  

[Deep reconstruction of strange attractors from time series](https://arxiv.org/pdf/2002.05909.pdf)

[Deep State Space Models for Time Series Forecasting](https://papers.nips.cc/paper/2018/file/5cf68969fb67aa6082363a6d4e6468e2-Paper.pdf)

